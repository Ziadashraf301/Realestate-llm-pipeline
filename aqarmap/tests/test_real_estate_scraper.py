import os
import warnings
import tensorflow as tf
from pathlib import Path
import sys

# Add project root to sys.path
PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(PROJECT_ROOT))

from src.real_estate_scraper import AQARMAPRealEstateScraper
import json

# Suppress unnecessary warnings
warnings.filterwarnings("ignore")
tf.get_logger().setLevel('ERROR')

# =============================================================================
# Configuration
# =============================================================================

# Use dynamic path resolution instead of hardcoded paths
CONFIG_DIR = PROJECT_ROOT / 'config'
GOOGLE_APPLICATION_CREDENTIALS = CONFIG_DIR / 'big_query_service_account.json'
TABLE_CONFIG_PATH = CONFIG_DIR / 'table_config.json'
LOG_FILE_PATH = PROJECT_ROOT / 'logs/aqarmap_scraper.log'

print("\n" + "=" * 70)
print("ğŸ“‚ Loading Configuration Files")
print("=" * 70)
print(f"ğŸ” Project Root: {PROJECT_ROOT}")
print(f"ğŸ” Config Directory: {CONFIG_DIR}")
print(f"ğŸ” Looking for: {TABLE_CONFIG_PATH}")
print(f"ğŸ” File exists: {TABLE_CONFIG_PATH.exists()}")
print("=" * 70 + "\n")

# Load Table Configuration
try:
    with open(TABLE_CONFIG_PATH, 'r', encoding='utf-8') as f:
        table_config = json.load(f)
    print(f"âœ… Table config loaded from: {TABLE_CONFIG_PATH}")

    GCP_PROJECT_ID = table_config.get('GCP_PROJECT_ID')
    BQ_DATASET_ID = table_config.get('BQ_DATASET_ID')
    BQ_TABLE_ID = table_config.get('BQ_TABLE_ID')

except FileNotFoundError:
    print(f"âš ï¸ WARNING: table_config.json not found at {TABLE_CONFIG_PATH}")
    print("   Using default fallback values...")
    GCP_PROJECT_ID = 'your-gcp-project-id'
    BQ_DATASET_ID = 'real_estate'
    BQ_TABLE_ID = 'scraped_properties'

# Validate Configuration
if not all([GCP_PROJECT_ID, BQ_DATASET_ID, BQ_TABLE_ID]):
    raise ValueError("âŒ Missing required configuration values in table_config.json")

if GCP_PROJECT_ID == 'your-gcp-project-id':
    raise ValueError("âŒ Please update GCP_PROJECT_ID in table_config.json with your actual project ID")

# Validate Credentials
if not GOOGLE_APPLICATION_CREDENTIALS.exists():
    raise FileNotFoundError(f"âŒ Missing credentials file: {GOOGLE_APPLICATION_CREDENTIALS}")

# Set Environment Variables
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(GOOGLE_APPLICATION_CREDENTIALS)
os.environ['GCP_PROJECT_ID'] = GCP_PROJECT_ID
os.environ['BQ_DATASET_ID'] = BQ_DATASET_ID
os.environ['BQ_TABLE_ID'] = BQ_TABLE_ID

print("\n" + "=" * 70)
print("ğŸ”§ Dagster Scraper Configuration")
print("=" * 70)
print(f"âœ… Credentials: {GOOGLE_APPLICATION_CREDENTIALS}")
print(f"ğŸ“Š GCP Project: {GCP_PROJECT_ID[:6]}****")
print(f"ğŸ“Š Dataset: {BQ_DATASET_ID}")
print(f"ğŸ“Š Table: {BQ_TABLE_ID}")
print("=" * 70 + "\n")

def main():
    """Main execution"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   Enhanced Egyptian Real Estate Scraper v2.0             â•‘
    â•‘   Features: URL tracking, logging, skip duplicates       â•‘
    â•‘   Storage: JSON + BigQuery                               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    
    scraper = AQARMAPRealEstateScraper(
        project_id=GCP_PROJECT_ID,
        dataset_id=BQ_DATASET_ID,
        table_id=BQ_TABLE_ID,
        log_file=LOG_FILE_PATH
    )
    
    # Configuration
    CITY = 'alexandria'           # cairo, alexandria, giza, etc.
    LISTING_TYPE = 'for-sale'     # for-sale or for-rent
    MAX_PAGES = 1                # Number of pages to scrape
    
    print(f"âš™ï¸  Configuration:")
    print(f"  â€¢ City: {CITY}")
    print(f"  â€¢ Type: {LISTING_TYPE}")
    print(f"  â€¢ Pages: {MAX_PAGES}")
    print(f"  â€¢ Deep scraping: ENABLED")
    print(f"  â€¢ URL tracking: ENABLED")
    print(f"  â€¢ Logging: ENABLED")
    print(f"  â€¢ BigQuery: {GCP_PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_ID}")
    print()
    
    try:
        # Scrape
        print("ğŸ”„ Starting scraping process...\n")
        scraper.scrape_aqarmap(
            city=CITY,
            listing_type=LISTING_TYPE,
            max_pages=MAX_PAGES
        )
        
        # Print summary
        scraper.print_summary()
        
        # Save results
        if scraper.results:
            print(f"\n{'='*60}")
            print("ğŸ’¾ Saving results...")
            print("="*60 + "\n")
            
            # Save to JSON
            json_file = 'aqarmap/raw_data/alexandria_for_sale.json'
            scraper.save_to_json(json_file)
            print(f"âœ… Saved to JSON: {json_file}")
            
            # Save to BigQuery
            try:
                inserted_count = scraper.save_to_bigquery()
                
                if inserted_count > 0:
                    print(f"âœ… Uploaded {inserted_count} properties to BigQuery")
                else:
                    print("â„¹ï¸  No new properties uploaded to BigQuery (all were duplicates)")
                    
            except Exception as bq_error:
                print(f"âš ï¸  BigQuery upload failed: {bq_error}")
                print("   Data is still saved in JSON format")
            
            print(f"\n{'='*60}")
            print("âœ… Scraping completed successfully!")
            print("ğŸ“ Files created:")
            print(f"   â€¢ {json_file}")
            print("   â€¢ logs/aqarmap_scraper.log (activity log)")
            if inserted_count > 0:
                print(f"   â€¢ BigQuery: {GCP_PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_ID}")
            print(f"{'='*60}\n")
        else:
            print("\nâš ï¸  No new properties found to save")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Scraping interrupted by user")
        print("ğŸ’¾ Saving partial results...")
        if scraper.results:
            scraper.save_to_json('aqarmap/raw_data/aqarmap_scraped_properties_detailed.json')
            print("âœ… Partial results saved")
        
    except Exception as e:
        print(f"\nâŒ Error during scraping: {e}")
        import traceback
        traceback.print_exc()
        
        # Try to save any results collected before the error
        if scraper.results:
            print("\nğŸ’¾ Attempting to save partial results...")
            try:
                scraper.save_to_json('raw_data/aqarmap_scraped_properties_detailed.json')
                print("âœ… Partial results saved")
            except:
                print("âŒ Failed to save partial results")


if __name__ == "__main__":
    # Check dependencies
    print("ğŸ” Checking dependencies...\n")
    
    missing_deps = []
    
    try:
        import requests
        print("âœ… requests")
    except ImportError:
        missing_deps.append("requests")
        print("âŒ requests")
    
    try:
        from bs4 import BeautifulSoup
        print("âœ… beautifulsoup4")
    except ImportError:
        missing_deps.append("beautifulsoup4")
        print("âŒ beautifulsoup4")
    
    try:
        from google.cloud import bigquery
        print("âœ… google-cloud-bigquery")
    except ImportError:
        missing_deps.append("google-cloud-bigquery")
        print("âŒ google-cloud-bigquery")
    
    if missing_deps:
        print(f"\nâš ï¸  Missing dependencies. Install with:")
        print(f"pip install {' '.join(missing_deps)}")
        exit(1)
    
    print("\nâœ… All dependencies installed\n")
    main()