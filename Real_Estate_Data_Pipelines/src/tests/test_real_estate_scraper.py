import warnings
from pathlib import Path
from src.scrapers import AQARMAPRealEstateScraper
from src.config import config
from src.databases import Big_Query_Database
from src.logger import LoggerFactory
from src.helpers import save_to_json, scraper_report


def test_scrapers_operations():
    """Execute the scraping pipeline with logging"""

    # Path Configuration
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
    OUTPUT_JSON = PROJECT_ROOT / "Real_Estate_Data_Pipelines" / "raw_data" / "alexandria_for_sale.json"

    # Load Config
    cfg = config

    # Initialize Logger
    logger = LoggerFactory.create_logger(log_dir=cfg.LOG_DIR)

    logger.info("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   ğŸ  Egyptian Real Estate Scraper v2.0                   â•‘
    â•‘   Features: URL tracking, structured logs, BQ storage    â•‘
    â•‘   Target: Raw dataset ingestion                          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    warnings.filterwarnings("ignore")

    logger.info(f"âœ… Loaded configuration")

    # Initialize BigQuery Database Client
    database = Big_Query_Database(
        project_id=cfg.GCP_PROJECT_ID,
        raw_dataset_id=cfg.BQ_RAW_DATASET_ID,
        raw_table_id=cfg.BQ_RAW_TABLE_ID,
        log_dir=cfg.LOG_DIR,
    )
    
    database.connect()

    # Initialize Scraper
    scraper = AQARMAPRealEstateScraper(
        log_dir=cfg.LOG_DIR,
        db=database
    )

    # Scraper Configuration
    CITY = "alexandria"
    LISTING_TYPE = "for-sale"
    MAX_PAGES = cfg.MAX_PAGES


    logger.info("âš™ï¸  Scraper Configuration:")
    logger.info(f"  â€¢ City: {CITY}")
    logger.info(f"  â€¢ Listing Type: {LISTING_TYPE}")
    logger.info(f"  â€¢ Max Pages: {MAX_PAGES}")
    logger.info(f"  â€¢ Deep scraping: ENABLED")
    logger.info(f"  â€¢ URL tracking: ENABLED\n")

    # START SCRAPING
    logger.info("ğŸš€ Starting AQARMAP scraping process...\n")

    try:
        scraper.scrape_aqarmap(
            city=CITY,
            listing_type=LISTING_TYPE,
            max_pages=MAX_PAGES
        )

        scraper_report(results=scraper.results, logger=logger)

        # SAVE RESULTS
        if scraper.results:
            logger.info("ğŸ’¾ Saving scraped properties...")

            # Save JSON
            save_to_json(filename=str(OUTPUT_JSON), results=scraper.results, logger=logger)
            logger.info(f"âœ… Saved JSON â†’ {OUTPUT_JSON}")

            # Save to BigQuery
            try:
                inserted_count = database.save_to_database(scraper.results)
                if inserted_count > 0:
                    logger.info(f"âœ… Inserted {inserted_count} new properties into BigQuery")
                else:
                    logger.info("â„¹ï¸  No new properties uploaded (all duplicates)")
            except Exception as upload_error:
                logger.warning(f"âš ï¸ BigQuery upload failed: {upload_error}")
                logger.info("   Data is still preserved in JSON")

            logger.info("ğŸ‰ Scraping completed successfully!")

        else:
            logger.warning("âš ï¸ No properties found to save")

    except KeyboardInterrupt:
        logger.warning("â›” Scraping manually interrupted!")
        if scraper.results:
            save_to_json(filename=str(OUTPUT_JSON), results=scraper.results, logger=logger)
            logger.info("ğŸ’¾ Partial results saved before exit")

    except Exception as e:
        logger.error(f"âŒ Unexpected scraper error: {e}")

        if scraper.results:
            try:
                fallback_path = OUTPUT_JSON.with_name("aqarmap_partial_fallback.json")
                save_to_json(filename=str(fallback_path), results=scraper.results, logger=logger)
                logger.info(f"ğŸ’¾ Partial results saved â†’ {fallback_path}")
            except:
                logger.error("âŒ Failed to save partial results")


if __name__ == "__main__":
    test_scrapers_operations()
