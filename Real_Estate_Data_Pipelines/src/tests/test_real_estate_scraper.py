import os
from pathlib import Path
from src.scrapers.aqarmap_real_estate_scraper import AQARMAPRealEstateScraper
import warnings
from src.config import Config
from src.databases import Big_Query_Database

def main():
    """Main execution"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   Enhanced Egyptian Real Estate Scraper v2.0             â•‘
    â•‘   Features: URL tracking, logging, skip duplicates       â•‘
    â•‘   Storage: JSON + BigQuery                               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # Suppress unnecessary warnings
    warnings.filterwarnings("ignore")

    PROJECT_ROOT = Path(__file__).resolve().parents[3]
    CONFIG_DIR = PROJECT_ROOT / "Configs"
    CONFIG_PATH = CONFIG_DIR / "Real_Estate_Data_Pipelines.json"
    print(CONFIG_PATH)
    GOOGLE_APPLICATION_CREDENTIALS = CONFIG_DIR / "big_query_service_account.json"

    # Load Config
    cfg = Config(CONFIG_PATH)

    # Set env variables dynamically
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(GOOGLE_APPLICATION_CREDENTIALS)
    os.environ["GCP_PROJECT_ID"] = cfg.GCP_PROJECT_ID
    os.environ["BQ_DATASET_ID"] = cfg.BQ_RAW_DATASET_ID
    os.environ["BQ_TABLE_ID"] = cfg.BQ_RAW_TABLE_ID

    # Log
    print("âœ… Configuration loaded dynamically from:", CONFIG_PATH)
    print(f"GCP_PROJECT_ID: {cfg.GCP_PROJECT_ID}")
    print(f"BQ_DATASET_ID: {cfg.BQ_RAW_DATASET_ID}")
    print(f"BQ_TABLE_ID: {cfg.BQ_RAW_TABLE_ID}")

    bg_database = Big_Query_Database(
        project_id=os.environ["GCP_PROJECT_ID"],
        dataset_id=os.environ["BQ_DATASET_ID"],
        table_id=os.environ["BQ_TABLE_ID"],
        log_dir=cfg.LOG_DIR, 
    )

    scraper = AQARMAPRealEstateScraper(
        log_dir=cfg.LOG_DIR,
        db_client=bg_database
    )
    
    # Configuration
    CITY = 'alexandria'           # cairo, alexandria, giza, etc.
    LISTING_TYPE = 'for-sale'     # for-sale or for-rent
    MAX_PAGES = 1                # Number of pages to scrape
    
    print(f"âš™ï¸  Configuration:")
    print(f"  â€¢ City: {CITY}")
    print(f"  â€¢ Type: {LISTING_TYPE}")
    print(f"  â€¢ Pages: {MAX_PAGES}")
    print(f"  â€¢ Deep scraping: ENABLED")
    print(f"  â€¢ URL tracking: ENABLED")
    print(f"  â€¢ Logging: ENABLED")
    print()
    
    try:
        # Scrape
        print("ğŸ”„ Starting scraping process...\n")
        scraper.scrape_aqarmap(
            city=CITY,
            listing_type=LISTING_TYPE,
            max_pages=MAX_PAGES
        )
        
        # Print summary
        scraper.print_summary()
        
        # Save results
        if scraper.results:
            print(f"\n{'='*60}")
            print("ğŸ’¾ Saving results...")
            print("="*60 + "\n")
            
            # Save to JSON
            json_file = 'C:/Users/MSI/OneDrive/Desktop/real_estate/Real_Estate_Data_Pipelines/raw_data/alexandria_for_sale.json'
            scraper.save_to_json(json_file)
            print(f"âœ… Saved to JSON: {json_file}")
            
            # Save to BigQuery
            try:
                inserted_count = bg_database.save_to_bigquery(scraper.results)
                
                if inserted_count > 0:
                    print(f"âœ… Uploaded {inserted_count} properties to BigQuery")
                else:
                    print("â„¹ï¸  No new properties uploaded to BigQuery (all were duplicates)")
                    
            except Exception as bq_error:
                print(f"âš ï¸  BigQuery upload failed: {bq_error}")
                print("   Data is still saved in JSON format")
            
            print(f"\n{'='*60}")
            print("âœ… Scraping completed successfully!")
            print("ğŸ“ Files created:")
            print(f"   â€¢ {json_file}")
            print("   â€¢ logs/aqarmap_scraper.log (activity log)")
            print(f"{'='*60}\n")
        else:
            print("\nâš ï¸  No new properties found to save")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Scraping interrupted by user")
        print("ğŸ’¾ Saving partial results...")
        if scraper.results:
            scraper.save_to_json('C:/Users/MSI/OneDrive/Desktop/real_estate/Real_Estate_Data_Pipelines/raw_data/aqarmap_scraped_properties_detailed.json')
            print("âœ… Partial results saved")
        
    except Exception as e:
        print(f"\nâŒ Error during scraping: {e}")
        import traceback
        traceback.print_exc()
        
        # Try to save any results collected before the error
        if scraper.results:
            print("\nğŸ’¾ Attempting to save partial results...")
            try:
                scraper.save_to_json('C:/Users/MSI/OneDrive/Desktop/real_estate/Real_Estate_Data_Pipelines/raw_data/aqarmap_scraped_properties_detailed.json')
                print("âœ… Partial results saved")
            except:
                print("âŒ Failed to save partial results")


if __name__ == "__main__":
    # Check dependencies
    print("ğŸ” Checking dependencies...\n")
    
    missing_deps = []
    
    try:
        import requests
        print("âœ… requests")
    except ImportError:
        missing_deps.append("requests")
        print("âŒ requests")
    
    try:
        from bs4 import BeautifulSoup
        print("âœ… beautifulsoup4")
    except ImportError:
        missing_deps.append("beautifulsoup4")
        print("âŒ beautifulsoup4")
    
    try:
        from google.cloud import bigquery
        print("âœ… google-cloud-bigquery")
    except ImportError:
        missing_deps.append("google-cloud-bigquery")
        print("âŒ google-cloud-bigquery")
    
    if missing_deps:
        print(f"\nâš ï¸  Missing dependencies. Install with:")
        print(f"pip install {' '.join(missing_deps)}")
        exit(1)
    
    print("\nâœ… All dependencies installed\n")
    main()